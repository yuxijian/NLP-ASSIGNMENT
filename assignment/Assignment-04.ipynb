{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "vector \u003d np.array([11231, 999, 123142])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "vector \u003d np.array([-10, 10, 24])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def softmax(vec):\n",
        "    vec -\u003d np.max(vec)\n",
        "    exp \u003d np.exp(vec)\n",
        "    return exp / np.sum(exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "softmax(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Assignment-04 基于维基百科的词向量构建"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "在本章，你将使用Gensim和维基百科获得你的第一批词向量，并且感受词向量的基本过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "![](https://www.kaggleusercontent.com/kf/1018109/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JNNggcCCDcYEypvp7ZDwOA.cM9CuDpuCKo0K_ZkMFLAUvhfip0P6SRZ4LddwgTtgwz8pQy1dZeGVJWi6u81KSpAFNSi7YximVVJbPw8xsFySdWlqoUwvSER-LLIRfmlpsCvtDt90NaLYT2FHlwl0tfF-1MKtiFsWlGQ8LGo40hL3ccBSwMZy214kGJf9bNkW_g.kZbF5sgN5qha3zhjilfSDg/__results___files/__results___9_0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Step-01: Download Wikipedia Chinese Corpus: https://dumps.wikimedia.org/zhwiki/20190720/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "第一步：使用维基百科下载中文语料库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Step-02: Using https://github.com/attardi/wikiextractor to extract the wikipedia corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "第二步：使用python wikipedia extractor抽取维基百科的内容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Step-03: Using gensim get word vectors: \n",
        "\n",
        "Reference: \n",
        "\n",
        "+ https://radimrehurek.com/gensim/models/word2vec.html\n",
        "+ https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "第三步：参考Gensim的文档和Kaggle的参考文档，获得词向量。 注意，你要使用Jieba分词把维基百科的内容切分成一个一个单词，然后存进新的文件中。然后，你需要用Gensim的LineSentence这个类进行文件的读取。\n",
        "\n",
        "在训练成词向量Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Step-04: Using some words to test your preformance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "第四步，测试同义词，找几个单词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Step-05: Using visualization tools: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "第五步：使用Kaggle给出的T-SEN进行词向量的可视化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Do homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 使用WikiExtractor.py 抽取文本信息"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "python3 WikiExtractor.py -b 500M -o zhwiki zhwiki-20190720-pages-articles-multistream.xml.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 繁体转换简体"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!pip3 install hanziconv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from hanziconv import hanziconv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "## 繁体转换成简体，因为之前jupter不知道怎么install hanziconv，所以在pycharm执行转化操作\n",
        "## 记：可以在pycharm中用anaconda的interpreter,然后import hanziconv,用工具自动安装\n",
        "def hanzoconv_trans():\n",
        "    path_dirs \u003d [\"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/wiki_0{}\".format(str(i)) for i in\n",
        "                 range(0, 3)]\n",
        "    new_paths \u003d [\"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/new_wiki_0{}\".format(str(i)) for i in\n",
        "                 range(0, 3)]\n",
        "    for i, path_dir in enumerate(path_dirs):\n",
        "        new_path \u003d new_paths[i]\n",
        "        new_file \u003d open(new_path, \u0027w\u0027, encoding\u003d\u0027utf-8\u0027)\n",
        "        count \u003d 0\n",
        "        with open(path_dir, \u0027r\u0027, encoding\u003d\u0027utf-8\u0027) as f:\n",
        "            for line in f.readlines():\n",
        "                print(\u0027translate count:\u0027 + str(count))\n",
        "                new_file.write(HanziConv.toSimplified(line))\n",
        "                count +\u003d 1\n",
        "\n",
        "        new_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 将3个简体文件数据的doc标签踢除，并合并到一个文件然后进行jieba分词"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "re_compile \u003d \u0027[^\u003cdoc.*\u003e$] | [^\u003c/doc\u003e$]\u0027\n",
        "path_dirs \u003d [\"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/test0{}\".format(str(i)) for i in\n",
        "                 range(1, 4)]\n",
        "new_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/wikitest\"\n",
        "new_file \u003d open(new_wikipedia, \u0027a+\u0027, encoding\u003d\u0027utf-8\u0027)\n",
        "count \u003d 1\n",
        "for i in path_dirs:\n",
        "    with open(i, \u0027r\u0027, encoding\u003d\u0027utf-8\u0027) as f:\n",
        "        for line in f.readlines():\n",
        "            content \u003d re.findall(\u0027[\\u4e00-\\u9fff]+\u0027, line)\n",
        "            new_file.write(\u0027\u0027.join(content))\n",
        "            print(\u0027write lines:\u0027+str(count))\n",
        "            count +\u003d1\n",
        "new_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 上面一个输入是测试只查找汉字正则，下面开始正文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "## 内存不够不要在jupter中尝试下面代码，请在pycharm中执行\n",
        "import re\n",
        "re_compile \u003d \u0027[^\u003cdoc.*\u003e$] | [^\u003c/doc\u003e$]\u0027\n",
        "path_dirs \u003d [\"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/wiki_0{}\".format(str(i)) for i in\n",
        "                 range(0, 3)]\n",
        "new_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/wiki_pure_text\"\n",
        "new_file \u003d open(new_wikipedia, \u0027a+\u0027, encoding\u003d\u0027utf-8\u0027)\n",
        "count \u003d 1\n",
        "for i in path_dirs:\n",
        "    with open(i, \u0027r\u0027, encoding\u003d\u0027utf-8\u0027) as f:\n",
        "        for line in f.readlines():\n",
        "            content \u003d re.findall(\u0027[\\u4e00-\\u9fff]+\u0027, line)\n",
        "            new_file.write(\u0027\u0027.join(content))\n",
        "            print(\u0027write lines:\u0027+str(count))\n",
        "            count +\u003d1\n",
        "            \n",
        "print(\u0027extract pure text finished!\u0027)\n",
        "new_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "##  使用jieba分词获取语料\n",
        "def jieba_ope():\n",
        "    \u0027\u0027\u0027jieba 分词处理\u0027\u0027\u0027\n",
        "    words_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/wiki_pure_text\"\n",
        "    corpus_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/corpus_wikipedia\"\n",
        "    corpus \u003d open(corpus_wikipedia, \u0027a+\u0027, encoding\u003d\u0027utf-8\u0027)\n",
        "    with open(words_wikipedia, \u0027r\u0027, encoding\u003d\u0027utf-8\u0027) as f:\n",
        "        content \u003d \u0027\u0027.join(f.readlines())\n",
        "        loop \u003d (len(content) // 100000) + 1\n",
        "        for i in range(loop):\n",
        "            begin \u003d 100000 * i\n",
        "            end \u003d begin + 100000\n",
        "            long \u003d content[begin:end]\n",
        "            corpus_temp \u003d jieba.lcut(long)\n",
        "            corpus.write(\u0027 \u0027.join(corpus_temp))\n",
        "            print(end)\n",
        "\n",
        "    corpus.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### gensim 获取词向量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim\u003d\u003d3.7.1 in /usr/local/lib/python3.7/site-packages (3.7.1)\n",
            "Requirement already satisfied: scipy\u003e\u003d0.18.1 in /usr/local/lib/python3.7/site-packages (from gensim\u003d\u003d3.7.1) (1.3.0)\n",
            "Requirement already satisfied: smart-open\u003e\u003d1.7.0 in /usr/local/lib/python3.7/site-packages (from gensim\u003d\u003d3.7.1) (1.8.4)\n",
            "Requirement already satisfied: numpy\u003e\u003d1.11.3 in /usr/local/lib/python3.7/site-packages (from gensim\u003d\u003d3.7.1) (1.16.3)\n",
            "Requirement already satisfied: six\u003e\u003d1.5.0 in /usr/local/lib/python3.7/site-packages (from gensim\u003d\u003d3.7.1) (1.12.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from smart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (1.9.201)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from smart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (2.22.0)\n",
            "Requirement already satisfied: boto\u003e\u003d2.32 in /usr/local/lib/python3.7/site-packages (from smart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (2.49.0)\n",
            "Requirement already satisfied: s3transfer\u003c0.3.0,\u003e\u003d0.2.0 in /usr/local/lib/python3.7/site-packages (from boto3-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (0.2.1)\n",
            "Requirement already satisfied: botocore\u003c1.13.0,\u003e\u003d1.12.201 in /usr/local/lib/python3.7/site-packages (from boto3-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (1.12.201)\n",
            "Requirement already satisfied: jmespath\u003c1.0.0,\u003e\u003d0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (0.9.4)\n",
            "Requirement already satisfied: idna\u003c2.9,\u003e\u003d2.5 in /usr/local/lib/python3.7/site-packages (from requests-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (2.8)\n",
            "Requirement already satisfied: chardet\u003c3.1.0,\u003e\u003d3.0.2 in /usr/local/lib/python3.7/site-packages (from requests-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (3.0.4)\n",
            "Requirement already satisfied: certifi\u003e\u003d2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (2019.3.9)\n",
            "Requirement already satisfied: urllib3!\u003d1.25.0,!\u003d1.25.1,\u003c1.26,\u003e\u003d1.21.1 in /usr/local/lib/python3.7/site-packages (from requests-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (1.25.3)\n",
            "Requirement already satisfied: docutils\u003c0.15,\u003e\u003d0.10 in /usr/local/lib/python3.7/site-packages (from botocore\u003c1.13.0,\u003e\u003d1.12.201-\u003eboto3-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (0.14)\n",
            "Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e\u003d2.1; python_version \u003e\u003d \"2.7\" in /usr/local/lib/python3.7/site-packages (from botocore\u003c1.13.0,\u003e\u003d1.12.201-\u003eboto3-\u003esmart-open\u003e\u003d1.7.0-\u003egensim\u003d\u003d3.7.1) (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gensim\u003d\u003d3.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\u0027/Users/yuxijian/PycharmProjects/NLP-ASSIGNMENT/assignment\u0027, \u0027/anaconda3/lib/python37.zip\u0027, \u0027/anaconda3/lib/python3.7\u0027, \u0027/anaconda3/lib/python3.7/lib-dynload\u0027, \u0027\u0027, \u0027/anaconda3/lib/python3.7/site-packages\u0027, \u0027/anaconda3/lib/python3.7/site-packages/aeosa\u0027, \u0027/anaconda3/lib/python3.7/site-packages/NetEase_MusicBox-0.2.5.4-py3.7.egg\u0027, \u0027/anaconda3/lib/python3.7/site-packages/IPython/extensions\u0027, \u0027/Users/yuxijian/.ipython\u0027]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from gensim.models import word2vec\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "## 训练预料\n",
        "def train_corpus():\n",
        "    corpus_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/corpus_wikipedia\"\n",
        "    model_wikipedia \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/corpus_wiki.model\"\n",
        "    logging.basicConfig(format\u003d\u0027%(asctime)s:%(levelname)s:%(message)s\u0027, level\u003dlogging.INFO)\n",
        "    sentences \u003d word2vec.LineSentence(corpus_wikipedia)\n",
        "    ## 设置词向量的维度为250\n",
        "    model \u003d word2vec.Word2Vec(sentences, size\u003d250)\n",
        "    model.save(model_wikipedia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 测试训练的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  \u0027See the migration notes for details: %s\u0027 % _MIGRATION_NOTES_URL\n",
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\u0027人工智慧\u0027, 0.785691499710083), (\u0027智能\u0027, 0.6901068687438965), (\u0027计算机\u0027, 0.6647605895996094), (\u0027专家系统\u0027, 0.6456795930862427), (\u0027神经网络\u0027, 0.6348234415054321), (\u0027机器人\u0027, 0.6309388279914856), (\u0027虚拟现实\u0027, 0.6223701238632202), (\u0027编程\u0027, 0.6215964555740356), (\u0027图像处理\u0027, 0.6173820495605469), (\u0027计算机技术\u0027, 0.6144931316375732)]\n"
          ]
        }
      ],
      "source": [
        "model_dir \u003d \"/Users/yuxijian/PycharmProjects/wikiextractor-master/zhwiki/AA/corpus_wiki.model\"\n",
        "model \u003d word2vec.Word2Vec.load(model_dir)\n",
        "result \u003d model.most_similar(\u0027人工智能\u0027, topn\u003d10)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\u0027试验\u0027, 0.7563213109970093), (\u0027验证\u0027, 0.6795560121536255), (\u0027检验\u0027, 0.6274953484535217), (\u0027调试\u0027, 0.6219578981399536), (\u0027实验\u0027, 0.6151777505874634), (\u0027试飞\u0027, 0.6085917949676514), (\u0027评估\u0027, 0.6055723428726196), (\u0027检测\u0027, 0.5936518907546997), (\u0027测谎\u0027, 0.5856833457946777), (\u0027试车\u0027, 0.5832120180130005)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "print(model.most_similar(\u0027测试\u0027, topn\u003d10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"word \u0027英雄联盟\u0027 not in vocabulary\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-5-fbc68498681e\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027英雄联盟\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 )\n\u001b[0;32m-\u003e 1447\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-\u003e 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 544\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word \u0027%s\u0027 not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word \u0027英雄联盟\u0027 not in vocabulary\""
          ]
        }
      ],
      "source": [
        "### disadvantage 模型预料不含有词汇,无法推荐相似\n",
        "print(model.most_similar(\u0027英雄联盟\u0027, topn\u003d10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "res1 \u003d model.similarity(\u0027女朋友\u0027,\u0027女票\u0027)\n",
        "res2 \u003d model.similarity(\u0027父亲\u0027,\u0027爸爸\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "res1 similarity:-0.033335667103528976; res2 similarity:0.5081502199172974\n"
          ]
        }
      ],
      "source": [
        "print(\u0027res1 similarity:{}; res2 similarity:{}\u0027.format(res1,res2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 7.1164332e-02, -1.4385771e+00, -6.8018800e-01, -7.5822675e-01,\n",
              "       -1.4495445e+00, -1.6373923e+00,  2.1643999e+00, -8.7514907e-01,\n",
              "       -7.4683934e-01,  1.5854133e+00, -4.8763230e-01,  1.4821599e+00,\n",
              "       -1.6753249e+00, -1.1516026e+00,  5.2499312e-01, -1.2749339e+00,\n",
              "       -1.0458238e+00, -1.9024004e+00, -1.9091805e+00,  1.1761414e+00,\n",
              "        6.8585324e-01,  1.6030023e+00,  8.0240637e-01,  4.2450267e-01,\n",
              "        5.3730595e-01,  5.8221614e-01,  8.3171266e-01, -1.3781707e+00,\n",
              "       -7.6930016e-01,  1.9129574e+00, -1.1741817e+00, -1.6063588e+00,\n",
              "       -5.7744098e-01,  8.1164479e-01, -1.3272020e+00,  3.5050547e-01,\n",
              "       -2.7435663e+00, -2.2316697e+00,  5.9924799e-01, -3.9834064e-01,\n",
              "       -1.5806755e+00, -1.1194706e+00, -4.2119005e-01,  7.2975165e-01,\n",
              "       -5.6066751e-01, -2.8058970e+00,  1.6532747e+00,  1.1075728e+00,\n",
              "        1.2259586e+00, -3.6794433e-01, -7.4976432e-01,  1.4166688e+00,\n",
              "        8.5579932e-01,  1.2155818e-01,  1.5668441e+00,  1.3317868e+00,\n",
              "       -5.7145530e-01, -7.8018606e-01, -6.2385112e-01, -8.0272734e-01,\n",
              "        1.9345510e+00, -3.1566978e+00, -1.4313192e+00, -7.5923347e-01,\n",
              "        5.4251099e-01, -1.2518889e+00, -1.5524832e-02,  1.2351394e+00,\n",
              "        5.7033896e-01, -8.5239790e-02,  7.1414524e-01,  1.6151584e+00,\n",
              "        2.2069693e+00,  1.3972464e+00, -1.2068979e+00,  5.8198261e-01,\n",
              "       -3.2279280e-03, -1.4731283e-01, -1.9618416e-02, -1.1941196e-01,\n",
              "        8.4848143e-02,  2.0197804e+00,  4.2153347e-01,  9.1791958e-01,\n",
              "       -2.0052207e+00,  1.0956893e+00, -2.5705510e-01, -1.8943503e-01,\n",
              "       -9.4283503e-01,  8.1538904e-01,  1.1716098e+00, -2.2428066e-01,\n",
              "       -7.0877445e-01,  1.9280901e+00,  1.4295650e+00, -8.2050139e-01,\n",
              "        1.5188860e+00, -2.3337140e+00, -1.1954698e-01,  3.3172684e+00,\n",
              "       -3.2348909e+00, -1.7353879e+00,  1.2596622e+00, -5.2876174e-01,\n",
              "        2.2523453e+00, -9.5603901e-01,  1.1415583e+00,  6.7777693e-02,\n",
              "        1.0786729e+00, -9.4138670e-01,  2.3443332e+00, -1.3738329e+00,\n",
              "        2.3219848e+00,  2.9565227e-01, -3.2329246e-02, -6.3520026e-01,\n",
              "        1.5774815e+00, -2.4981956e-01, -2.6777973e+00, -8.5193133e-01,\n",
              "        2.4543202e+00,  1.6514467e+00,  1.9516339e+00,  4.2358987e-02,\n",
              "        2.1094387e+00, -1.3969733e+00, -2.3960373e+00,  1.6656535e+00,\n",
              "       -7.5720837e-03,  5.5579394e-01, -1.1159472e+00, -6.1554898e-02,\n",
              "        6.1539739e-01, -4.5076376e-01, -5.0656390e-01, -8.8372552e-01,\n",
              "       -1.1555088e+00, -6.8611217e-01, -1.6770238e+00,  1.5082910e+00,\n",
              "       -5.8765900e-01, -3.7985831e-01,  1.3371840e+00,  7.9470915e-01,\n",
              "       -5.8778429e-01, -4.5214790e-01, -1.9173939e+00, -4.4320098e-01,\n",
              "        6.8619978e-01,  3.9440215e+00, -1.1368316e+00,  1.0800351e+00,\n",
              "        2.2212358e+00,  1.6059844e-01, -3.1563299e+00,  1.3132980e+00,\n",
              "       -2.2370574e+00, -4.7771251e-01, -5.3206640e-01,  1.4246042e+00,\n",
              "       -9.0201855e-01, -2.5404232e+00,  1.1466178e+00,  2.4816838e-01,\n",
              "        2.7589419e+00,  1.1627423e+00, -2.6369035e-01,  6.9557321e-01,\n",
              "       -2.1304924e+00, -2.4717171e+00, -5.9573218e-02, -1.7285492e-01,\n",
              "        8.8732100e-01, -6.0978878e-01,  6.8558022e-02, -1.5348287e-01,\n",
              "       -6.4481771e-01, -1.3001763e+00,  2.1546414e+00, -2.7408664e+00,\n",
              "       -1.6891572e+00,  1.1984296e+00,  1.4026816e+00, -1.0990615e+00,\n",
              "        5.3538316e-01,  1.0020943e+00, -2.2892947e+00,  2.4017041e+00,\n",
              "        1.4626898e-01, -3.5171542e-02,  4.3279957e-02, -1.1631513e-01,\n",
              "        1.3797066e+00,  7.3732281e-01, -2.6297054e+00,  1.1717911e+00,\n",
              "        1.1447548e+00,  7.1921355e-01, -4.1332474e-01,  7.4083763e-01,\n",
              "       -6.7930412e-01,  2.0935011e+00, -1.9342326e+00,  5.6372088e-01,\n",
              "       -1.4244343e+00, -3.1650281e+00,  1.4213611e+00, -1.2235361e-01,\n",
              "       -2.5996244e-01,  1.8825505e+00, -1.7835895e-02, -1.5587370e+00,\n",
              "       -5.1852417e-01, -1.6242105e-01, -1.9514453e+00, -5.6371301e-01,\n",
              "       -1.8621603e+00,  5.5966580e-01,  1.1641097e+00,  8.4053504e-01,\n",
              "        1.1212447e+00, -3.0451250e-01, -3.4028950e-01, -1.3558697e+00,\n",
              "       -2.8624122e+00,  1.1530359e+00,  6.4916068e-01,  1.4693217e+00,\n",
              "       -2.5843182e-01,  2.4206402e+00,  2.1295571e+00, -5.4186374e-01,\n",
              "        1.4861027e+00,  3.2131723e-01,  1.4776719e+00, -3.1075780e+00,\n",
              "        1.9351778e+00,  2.9388839e-01,  7.8572291e-01,  2.6232550e+00,\n",
              "       -1.6554679e+00,  9.9592054e-01,  9.5955414e-01, -6.8139011e-01,\n",
              "       -9.0306915e-02, -1.5888877e-02,  3.8653138e-01, -7.9492956e-01,\n",
              "       -6.9746131e-01,  2.5246892e+00], dtype\u003dfloat32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[\u0027数学\u0027]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### visualization tools "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# A more selective model\n",
        "model \u003d word2vec.Word2Vec(corpus, size\u003d100, window\u003d20, min_count\u003d500, workers\u003d4)\n",
        "tsne_plot(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "you must first build vocabulary before training the model",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-14-8438ff8c53ae\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u0027trump\u0027\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 783\u001b[0;31m             fast_version\u003dFAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 763\u001b[0;31m                 end_alpha\u003dself.min_alpha, compute_loss\u003dcompute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 910\u001b[0;31m             queue_factor\u003dqueue_factor, report_delay\u003dreport_delay, compute_loss\u003dcompute_loss, callbacks\u003dcallbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 536\u001b[0;31m             total_words\u003dtotal_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ],
      "source": [
        "model \u003d word2vec.Word2Vec(corpus, size\u003d100, window\u003d20, min_count\u003d200, workers\u003d4)\n",
        "model.wv[\u0027trump\u0027]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def tsne_plot(model):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels \u003d []\n",
        "    tokens \u003d []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model \u003d TSNE(perplexity\u003d40, n_components\u003d2, init\u003d\u0027pca\u0027, n_iter\u003d2500, random_state\u003d23)\n",
        "    new_values \u003d tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x \u003d []\n",
        "    y \u003d []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize\u003d(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy\u003d(x[i], y[i]),\n",
        "                     xytext\u003d(5, 2),\n",
        "                     textcoords\u003d\u0027offset points\u0027,\n",
        "                     ha\u003d\u0027right\u0027,\n",
        "                     va\u003d\u0027bottom\u0027)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "source": "## project01 : news speech extractor\n### project address : git@github.com:yuxijian/NLP-ASSIGNMENT.git",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}